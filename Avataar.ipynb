{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPQx+kbRS2xq9D+QkakIPQW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mahakgarg333/Avataar/blob/main/Avataar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 1: Segment Object Code"
      ],
      "metadata": {
        "id": "9guu4Sk5xKSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision segment-anything diffusers transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9w7qB5SexMcx",
        "outputId": "ee2249b9-3cd4-466d-ff2d-854a43d5fcda"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.19.1+cu121)\n",
            "Collecting segment-anything\n",
            "  Downloading segment_anything-1.0-py3-none-any.whl.metadata (487 bytes)\n",
            "Collecting diffusers\n",
            "  Downloading diffusers-0.30.3-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from diffusers) (8.4.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.24.7)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from diffusers) (0.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers) (3.20.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Downloading segment_anything-1.0-py3-none-any.whl (36 kB)\n",
            "Downloading diffusers-0.30.3-py3-none-any.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: segment-anything, diffusers\n",
            "Successfully installed diffusers-0.30.3 segment-anything-1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMY07h5oaJw1",
        "outputId": "96ff2d95-0278-4f76-a065-6185aea28d27"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-04 09:24:22--  https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 18.238.136.37, 18.238.136.120, 18.238.136.52, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|18.238.136.37|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2564550879 (2.4G) [binary/octet-stream]\n",
            "Saving to: ‘sam_vit_h_4b8939.pth’\n",
            "\n",
            "sam_vit_h_4b8939.pt 100%[===================>]   2.39G   107MB/s    in 25s     \n",
            "\n",
            "2024-10-04 09:24:47 (98.3 MB/s) - ‘sam_vit_h_4b8939.pth’ saved [2564550879/2564550879]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_sam_model():\n",
        "    sam_checkpoint = \"/content/sam_vit_h_4b8939.pth\"  # Correct path to the downloaded file\n",
        "    model_type = \"vit_h\"\n",
        "    sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "    return sam\n"
      ],
      "metadata": {
        "id": "nhpY8f1PaJ0Y"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YBSgz7q4el6W",
        "outputId": "0e310a4d-2f3b-4ec7-a391-afe51c8b75c6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chair.jpg  sample_data\tsam_vit_h_4b8939.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Load the SAM model\n",
        "def load_sam_model():\n",
        "    sam_checkpoint = \"/content/sam_vit_h_4b8939.pth\"  # Correct path after downloading the model\n",
        "    model_type = \"vit_h\"\n",
        "    sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "    return sam\n",
        "\n",
        "def segment_object(image_path, object_class, output_path):\n",
        "    # Load SAM model\n",
        "    sam = load_sam_model()\n",
        "    predictor = SamPredictor(sam)\n",
        "\n",
        "    # Read the image\n",
        "    image = cv2.imread(image_path)\n",
        "    predictor.set_image(image)\n",
        "\n",
        "    # Dummy object prompt for bounding box (Replace with actual detection model for the object class)\n",
        "    # This example uses a static bounding box; in a real scenario, you would replace it with automatic detection\n",
        "    box = np.array([50, 50, 300, 300])  # Example bounding box for testing\n",
        "\n",
        "    # Predict mask for the detected object\n",
        "    masks, _, _ = predictor.predict(box=box)\n",
        "\n",
        "    # Apply the mask on the image\n",
        "    mask_image = np.zeros_like(image)\n",
        "    mask_image[masks[0]] = [0, 0, 255]  # Red color mask for the object\n",
        "\n",
        "    # Blend original image and mask\n",
        "    output_image = cv2.addWeighted(image, 0.7, mask_image, 0.3, 0)\n",
        "\n",
        "    # Save the output\n",
        "    cv2.imwrite(output_path, output_image)\n",
        "    print(f\"Segmentation result saved to {output_path}\")\n",
        "\n",
        "# Example usage\n",
        "image_path = '/content/chair.jpg'  # Path to your input image\n",
        "object_class = 'chair'  # The class of the object you want to segment\n",
        "output_path = '/content/segmented.png'  # Output file path\n",
        "\n",
        "segment_object(image_path, object_class, output_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8QPkTgxwLCs",
        "outputId": "b0e72c16-77f5-4b1d-b468-733bdeb1a41e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/segment_anything/build_sam.py:105: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(f)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation result saved to /content/segmented.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2: Pose Editing Code"
      ],
      "metadata": {
        "id": "cTx3remyxWZN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Load the SAM model\n",
        "def load_sam_model():\n",
        "    sam_checkpoint = \"/content/sam_vit_h_4b8939.pth\"  # Correct path after downloading the model\n",
        "    model_type = \"vit_h\"\n",
        "    sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "    return sam\n",
        "\n",
        "def segment_object(image_path, object_class, output_path, azimuth_angle=0, polar_angle=0):\n",
        "    # Load SAM model\n",
        "    sam = load_sam_model()\n",
        "    predictor = SamPredictor(sam)\n",
        "\n",
        "    # Read the image\n",
        "    image = cv2.imread(image_path)\n",
        "    predictor.set_image(image)\n",
        "\n",
        "    # Dummy object prompt for bounding box (Replace with actual detection model for the object class)\n",
        "    # This example uses a static bounding box; in a real scenario, you would replace it with automatic detection\n",
        "    box = np.array([50, 50, 300, 300])  # Example bounding box for testing\n",
        "\n",
        "    # Predict mask for the detected object\n",
        "    masks, _, _ = predictor.predict(box=box)\n",
        "\n",
        "    # Apply the mask on the image\n",
        "    mask_image = np.zeros_like(image)\n",
        "    mask_image[masks[0]] = [0, 0, 255]  # Red color mask for the object\n",
        "\n",
        "    # Get the object mask\n",
        "    object_mask = np.zeros_like(image)\n",
        "    object_mask[masks[0]] = image[masks[0]]\n",
        "\n",
        "    # Rotate the object\n",
        "    (h, w) = object_mask.shape[:2]\n",
        "    center = (w // 2, h // 2)\n",
        "    M = cv2.getRotationMatrix2D(center, azimuth_angle, 1.0)\n",
        "    rotated_object = cv2.warpAffine(object_mask, M, (w, h))\n",
        "\n",
        "    # Create a mask for the rotated object\n",
        "    rotated_mask = np.zeros_like(rotated_object)\n",
        "    rotated_mask[rotated_object != 0] = 255\n",
        "\n",
        "    # Remove the rotated object from the original image\n",
        "    image_without_object = np.copy(image)\n",
        "    image_without_object[masks[0]] = 0\n",
        "\n",
        "    # Add the rotated object to the original image\n",
        "    output_image = np.copy(image_without_object)\n",
        "    output_image[rotated_mask != 0] = rotated_object[rotated_mask != 0]\n",
        "\n",
        "    # Save the output\n",
        "    cv2.imwrite(output_path, output_image)\n",
        "    print(f\"Segmentation result saved to {output_path}\")\n",
        "\n",
        "# Example usage\n",
        "image_path = '/content/chair.jpg'  # Path to your input image\n",
        "object_class = 'chair'  # The class of the object you want to segment\n",
        "output_path = '/content/rotated.png'  # Output file path\n",
        "azimuth_angle = 72  # Azimuth angle of rotation\n",
        "polar_angle = 0  # Polar angle of rotation (not used in this example)\n",
        "\n",
        "segment_object(image_path, object_class, output_path, azimuth_angle, polar_angle)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ne-H0VAn7Bll",
        "outputId": "c3be0e9b-0278-4b4e-eaa6-356594d04121"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation result saved to /content/rotated.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# Load the SAM model\n",
        "def load_sam_model():\n",
        "    sam_checkpoint = \"/content/sam_vit_h_4b8939.pth\"  # Correct path after downloading the model\n",
        "    model_type = \"vit_h\"\n",
        "    sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
        "    return sam\n",
        "\n",
        "def segment_object(image_path, object_class, output_path, azimuth_angle=0, polar_angle=0):\n",
        "    # Load SAM model\n",
        "    sam = load_sam_model()\n",
        "    predictor = SamPredictor(sam)\n",
        "\n",
        "    # Read the image\n",
        "    image = cv2.imread(image_path)\n",
        "    predictor.set_image(image)\n",
        "\n",
        "    # Dummy object prompt for bounding box (Replace with actual detection model for the object class)\n",
        "    # This example uses a static bounding box; in a real scenario, you would replace it with automatic detection\n",
        "    box = np.array([50, 50, 300, 300])  # Example bounding box for testing\n",
        "\n",
        "    # Predict mask for the detected object\n",
        "    masks, _, _ = predictor.predict(box=box)\n",
        "\n",
        "    # Apply the mask on the image\n",
        "    mask_image = np.zeros_like(image)\n",
        "    mask_image[masks[0]] = [0, 0, 255]  # Red color mask for the object\n",
        "\n",
        "    # Get the object mask\n",
        "    object_mask = np.zeros_like(image)\n",
        "    object_mask[masks[0]] = image[masks[0]]\n",
        "\n",
        "    # Get the bounding box of the object\n",
        "    x, y, w, h = box\n",
        "\n",
        "    # Crop the object from the original image\n",
        "    object_image = image[y:y+h, x:x+w]\n",
        "\n",
        "    # Rotate the object\n",
        "    (h, w) = object_image.shape[:2]\n",
        "    center = (w // 2, h // 2)\n",
        "    M = cv2.getRotationMatrix2D(center, azimuth_angle, 1.0)\n",
        "    rotated_object = cv2.warpAffine(object_image, M, (w, h))\n",
        "\n",
        "    # Create a new image with the same size as the original image\n",
        "    new_image = np.copy(image)\n",
        "\n",
        "    # Remove the original object from the new image\n",
        "    new_image[y:y+h, x:x+w] = 0\n",
        "\n",
        "    # Get the bounding box of the rotated object\n",
        "    (h, w) = rotated_object.shape[:2]\n",
        "    x = (image.shape[1] - w) // 2\n",
        "    y = (image.shape[0] - h) // 2\n",
        "\n",
        "    # Add the rotated object to the new image\n",
        "    new_image[y:y+h, x:x+w] = rotated_object\n",
        "\n",
        "    # Save the output\n",
        "    cv2.imwrite(output_path, new_image)\n",
        "    print(f\"Segmentation result saved to {output_path}\")\n",
        "\n",
        "# Example usage\n",
        "image_path = '/content/chair.jpg'  # Path to your input image\n",
        "object_class = 'chair'  # The class of the object you want to segment\n",
        "output_path = '/content/rotated.png'  # Output file path\n",
        "azimuth_angle = 72  # Azimuth angle of rotation\n",
        "polar_angle = 0  # Polar angle of rotation (not used in this example)\n",
        "\n",
        "segment_object(image_path, object_class, output_path, azimuth_angle, polar_angle)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FMIxMJ9j_JL-",
        "outputId": "260b1dc2-a713-4a76-c8b0-2d73525b330a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segmentation result saved to /content/rotated.png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4sgTHFQ-7Brd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}